# BigQuery Migration Automation Tool

The BigQuery Migration Automation Tool is used to batch transpile all of the Simba SQLs and push those transpiled SQLs to github.
## Architecture 

![Simba Risk Mitigation drawio](https://github.com/RealistAI/bq_migration_automation_tool/assets/99982739/7a40c2bd-fff9-4a35-b735-a614f320a5e7)

## Tools Used
BQ Migration CLI - CLI tooling built by the BigQuery Migration Service team that allows you to interact with the BigQuery Migration Service.

Makefile- A Makefile is used to run the python scripts, the pip installs and all the requirements needed to run the BigQuery Migration Automation Tool.

## Required repositories 
UC4 SQL Repo\
This is the repository of all of the validated SQLs generated by the BigQuery Migration 
Service tool. The tool uploads the verified SQL to this repository.

Folder Structure:
<pre>
|-- Teradata SQLs
    |--Sub Folders
        |-- my_sql.sql
        â€¦ 
|-- BigQuery SQLs
    |--Sub Folders
        |-- my_sql.sql
</pre>

The Teradata SQLs folder contains all of the Teradata SQLs for the UC4 Jobs.
The BigQuery SQLs folder contains all of the converted BigQuery SQLs for the UC4 Jobs

DWH Migration Tools Repo\
This is the Github repository that contains the dwh-migration-tools that is required 
for the transpilation of the Teradata SQL

## Setup
The first part of the Makefile will run the setup.py. This script will clone the required repos 
into the local file system, if the given Github repo exists already in our local file system, we will 
do a git pull instead.

## Transpilation
The transpilation is completed using `bqms-run`. The script sets the environment variables required by 
the BQMS tool and then run the `bqms-run` command to initilize the transpilation process. 

## Dry Run
We then iterate through the files in the BQMS output submit a dry run query for each of them.
If the query is successful the file will then be moved into the UC4_SQL_REPO in the bigquery_sql/ 
directory.

## Failure Logs
If any of the BigQuery dry runs fails, it creates a CSV file in the `failure_logs/` directory that 
contains the name of the file that failed the dry run query, the type of error it failed with, the 
error message, and the timestamp of the run itself.

## Github Integration
Upon completion of the validaiton process, the script will create a new branch in the origin repository, 
and push the updated UC4 SQL. 


## Usage
In order to utilize this tool, you first need to clone the project into the directory of your choice 
`git clone https://github.com/RealistAI/bq_migration_automation_tool.git`, navigate into the newly cloned 
directory `cd bq_migration_automation_tool`, and alter the config.py to your own specification. Create 
a Pip virtual environment using `pipenv shell` and install the required libraries `pipenv install`, 
and run the Makefile `make`

## config.py

##### DWH_MIGRATION_TOOL_REPO
The repository url and branch containing the dwh-migration-tools


##### UC4_SQL_REPO
The repository url and branch containing the SQL's to translate


##### PROJECT
The name of the Google Cloud Platform project that will performing the bulk translation


##### PREPROCESED_BUCKET
A Google Cloud Storage bucket that will be used by `bqms-run` as a staging area for the translation process


##### SOURCE_SQL_PATH
The directory in your Github repository containing .sql files for translation and validation


##### TRANSLATED_BUCKET
A Google Cloud Storage bucket that will be used by `bqms-run` to store translated files before dumping 
them back into the local file system.


##### SQL_TO_VALIDATE
The local directory that `bqms-run` will use to store the results of the run.


##### TARGET_SQL_PATH
The directory within the origin Github repository to contain the translated and validated .sql files


##### CONFIG
The path to the dwh-migration-tools config.yaml file. 


##### MAPPING_CONFIG_FILE
The path to any given object name mapping configuration files.


##### FAILURE_LOGS
The target destination for the failure logs contructed during the validation process.

